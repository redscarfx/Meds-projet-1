{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d86c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fdf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def build_texts_from_tsv_list(tsv_paths, base_dir=\"../../WikiTableQuestions/WikiTableQuestions\"):\n",
    "    all_texts = []\n",
    "    all_ids = []\n",
    "    all_rows = []   # <-- on garde aussi toutes les infos utiles\n",
    "\n",
    "    for tsv_path in tsv_paths:\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(base_dir, \"data\", tsv_path),\n",
    "            sep=\"\\t\",\n",
    "            dtype=str,\n",
    "            engine=\"python\",\n",
    "            on_bad_lines=\"skip\"\n",
    "        ).fillna(\"\")\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df),\n",
    "                           desc=f\"Construction des textes ({tsv_path})\"):\n",
    "\n",
    "            question = str(row[\"utterance\"])\n",
    "            context_path = row[\"context\"].strip()\n",
    "            full_path = os.path.join(base_dir, context_path)\n",
    "\n",
    "            try:\n",
    "                table_df = pd.read_csv(full_path, dtype=str, engine=\"python\",\n",
    "                                       on_bad_lines=\"skip\").fillna(\"\")\n",
    "\n",
    "                table_header = \" @ \".join(table_df.columns)\n",
    "                table_header += \"\\n\"\n",
    "\n",
    "                table_lines = [table_header]\n",
    "                for _, table_row in table_df.iterrows():\n",
    "                    row_text = \" @ \".join(table_row)\n",
    "                    table_lines.append(f\"{row_text} \\n\")\n",
    "\n",
    "                full_text = \" \".join(table_lines)\n",
    "\n",
    "                all_texts.append(full_text)\n",
    "                all_ids.append(row[\"id\"])\n",
    "\n",
    "                # on garde aussi les vraies colonnes brutes pour l’annotation\n",
    "                all_rows.append({\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"utterance\": row[\"utterance\"],\n",
    "                    \"context\": row[\"context\"],\n",
    "                    \"targetValue\": row[\"targetValue\"],\n",
    "                    \"table\": full_text\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du chargement de {full_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"{len(all_texts)} exemples valides construits.\")\n",
    "    return all_rows  # <-- on retourne directement les données complètes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construction des textes (training.tsv): 100%|██████████| 14111/14111 [02:36<00:00, 89.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14111 exemples valides construits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Construction des textes (random-split-1-dev.tsv): 100%|██████████| 2806/2806 [00:31<00:00, 88.61it/s] \n",
      "Construction des textes (random-split-2-dev.tsv): 100%|██████████| 2833/2833 [00:32<00:00, 86.65it/s] \n",
      "Construction des textes (random-split-3-dev.tsv): 100%|██████████| 2830/2830 [00:33<00:00, 84.06it/s] \n",
      "Construction des textes (random-split-4-dev.tsv): 100%|██████████| 2820/2820 [00:32<00:00, 87.96it/s] \n",
      "Construction des textes (random-split-5-dev.tsv): 100%|██████████| 2825/2825 [00:31<00:00, 88.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14114 exemples valides construits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if FIRST_RUN:\n",
    "\ttrain_rows = build_texts_from_tsv_list([\"training.tsv\"])\n",
    "\tsubset_train = random.sample(train_rows, 200)\n",
    "\tdf_train = pd.DataFrame(subset_train)\n",
    "\tdf_train.to_csv(\"subset_training.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "\ttest_rows  = build_texts_from_tsv_list([\n",
    "\t\t\"random-split-1-dev.tsv\", \"random-split-2-dev.tsv\",\n",
    "\t\t\"random-split-3-dev.tsv\", \"random-split-4-dev.tsv\",\n",
    "\t\t\"random-split-5-dev.tsv\"\n",
    "\t])\n",
    "\tsubset_test  = random.sample(test_rows, 200)\n",
    "\tdf_test  = pd.DataFrame(subset_test)\n",
    "\tdf_test.to_csv(\"subset_testing.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a39725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: (20, 1635)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGG       0.21      0.60      0.32         5\n",
      "        ARTH       0.00      0.00      0.00         3\n",
      "        COMP       0.00      0.00      0.00         3\n",
      "      LOOKUP       0.00      0.00      0.00         4\n",
      "        Next       0.00      0.00      0.00         1\n",
      "       SUPER       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.20        20\n",
      "   macro avg       0.20      0.14      0.12        20\n",
      "weighted avg       0.25      0.20      0.16        20\n",
      "\n",
      "[[3 0 0 2 0 0]\n",
      " [3 0 0 0 0 0]\n",
      " [2 0 0 1 0 0]\n",
      " [3 1 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [2 0 0 1 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yacin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\yacin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\yacin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['semantic_classifier_tfidf.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"subset_labeled.tsv\", sep=\"\\t\")\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "X_train = train_df[\"utterance\"]+\" - \"+train_df[\"context\"]\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test = test_df[\"utterance\"]+\" - \"+test_df[\"context\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=None, ngram_range=(2,3))\t \n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=3000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "joblib.dump((tfidf, clf), \"semantic_classifier_tfidf.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29f2740c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: Le module spécifié est introuvable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32mc:\\Users\\yacin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:239\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m    238\u001b[0m         _load_global_deps()\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _C: Le module spécifié est introuvable."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        self.texts = (df[\"utterance\"]+\" - \"+df[\"context\"]).tolist()\n",
    "        self.labels = df[\"label\"].astype(\"category\")\n",
    "        self.label_ids = self.labels.cat.codes\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_map = dict(enumerate(self.labels.cat.categories))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.label_ids.iloc[idx])\n",
    "        return item\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Charger votre fichier unique de 100 exemples\n",
    "df = pd.read_csv(\"subset_labeled.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 2) Train/test internes (20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42,\n",
    "                                     stratify=df[\"label\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Préparation des données\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_ds = TextDataset(train_df, tokenizer)\n",
    "test_ds = TextDataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=8)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Modèle\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(train_df[\"label\"].unique())\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Entraînement\n",
    "for epoch in range(5):  # 5 epochs = idéal pour si peu de données\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"[Epoch {epoch+1}] Loss = {loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6) Évaluation\n",
    "model.eval()\n",
    "preds, gold = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        labels = batch[\"labels\"]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits.cpu()\n",
    "        preds.extend(logits.argmax(dim=1).tolist())\n",
    "        gold.extend(labels.tolist())\n",
    "\n",
    "label_names = train_df[\"label\"].astype(\"category\").cat.categories\n",
    "print(classification_report(gold, preds, target_names=label_names))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7) Sauvegarde\n",
    "model.save_pretrained(\"semantic_classifier_distilbert\")\n",
    "tokenizer.save_pretrained(\"semantic_classifier_distilbert\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
