{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GWh6K24BmU9U",
        "outputId": "b2873d1e-bce0-4641-c340-641547463f97",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed transformers-4.57.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "b7799eb30be34b11bb067473277b31e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5UUdPsxoCtW",
        "outputId": "9fe77fc6-d931-43e8-b954-a1a464fc1562",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_and_flatten_tsv(path: str) -> str:\n",
        "    try:\n",
        "        df_tsv = pd.read_csv(path, sep=\"\\t\")\n",
        "    except FileNotFoundError:\n",
        "        return f\"[Fichier introuvable: {path}]\"\n",
        "\n",
        "    # Conversion du tableau en texte : chaque ligne devient une phrase\n",
        "    rows = []\n",
        "    for _, row in df_tsv.iterrows():\n",
        "        # Format : \"col1: val1 | col2: val2 | ...\"\n",
        "        row_text = \" | \".join(f\"{col}: {row[col]}\" for col in df_tsv.columns)\n",
        "        rows.append(row_text)\n",
        "\n",
        "    # Contexte final : concaténation\n",
        "    return \"\\n\".join(rows)\n",
        "\n",
        "\n",
        "# --- Chargement du DataFrame principal ---\n",
        "df = pd.read_csv(\"subset_labeled.tsv\", sep='\\t')  # ou ton fichier principal\n",
        "\n",
        "# Le répertoire racine du dataset (où se trouvent les TSV)\n",
        "#base_dir = os.path.dirname(\"train.csv\")  # à adapter si nécessaire\n",
        "\n",
        "# --- Traitement de la colonne \"context\" ---\n",
        "flattened_contexts = []\n",
        "\n",
        "for rel_path in df[\"context\"]:\n",
        "    tsv_path = rel_path.replace(\".csv\", \".tsv\")\n",
        "    tsv_path = os.path.abspath(tsv_path)\n",
        "\n",
        "    text_context = load_and_flatten_tsv(tsv_path)\n",
        "    flattened_contexts.append(text_context)\n",
        "\n",
        "df[\"context\"] = flattened_contexts\n",
        "print(\"file read\\n\")\n",
        "# disply first row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "HkXn7RGmvAPm",
        "outputId": "29907519-ff67-41da-8297-d907a2281631"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file read\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ceremony: 2011 Edison Awards | Award: Edison Award | Category: Best Romantic Movie | Name: nan | Outcome: Won\\nCeremony: 2011 Vijay Music Awards | Award: Vijay Music Award | Category: Best Folk Song of the Year 2010 | Name: Adida Nayandiya | Outcome: Nominated\\nCeremony: 2011 Vijay Music Awards | Award: Vijay Music Award | Category: Popular Melody of the Year 2010 | Name: Idhu Varai | Outcome: Nominated\\nCeremony: 2011 Vijay Music Awards | Award: Vijay Music Award | Category: Popular Duet of the Year 2010 | Name: Andrea Jeremiah & Ajeesh for Idhu Varai | Outcome: Won\\nCeremony: 2011 Vijay Music Awards | Award: Vijay Music Award | Category: Popular Female Singer of the Year 2010 | Name: Andrea Jeremiah for Idhu Varai | Outcome: Nominated\\nCeremony: 2011 Vijay Music Awards | Award: Vijay Music Award | Category: Best Debut Male Playback Singer (Jury) | Name: Ajeesh | Outcome: Nominated\\nCeremony: 5th Vijay Awards | Award: Vijay Award | Category: Vijay Award for Best Supporting Actor | Name: Sampath Raj | Outcome: Nominated\\nCeremony: 5th Vijay Awards | Award: Vijay Award | Category: Vijay Award for Best Female Playback Singer | Name: Andrea Jeremiah | Outcome: Nominated\\nCeremony: 5th Vijay Awards | Award: Vijay Award | Category: Vijay Award for Best Lyricist | Name: Gangai Amaran | Outcome: Nominated\\nCeremony: Mirchi Music Awards | Award: Mirchi Music Award | Category: Mirchi Listeners’ Choice – Best Song of the Year | Name: Idhu Varai | Outcome: Nominated\\nCeremony: Mirchi Music Awards | Award: Mirchi Music Award | Category: Technical – Sound Mixer | Name: Ramji & Guru for Idhu Varai | Outcome: Won\\nCeremony: Mirchi Music Awards | Award: Mirchi Music Award | Category: Best Upcoming Singer of the Year – Male | Name: Ajeesh for Idhu Varai | Outcome: Won'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILWHr5NsluD4",
        "outputId": "b864081c-e8ff-4a33-e6fa-366d47d07274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier principal chargé : (100, 5)\n",
            "Aplatissement des contextes…\n",
            "Contexte aplati ajouté au dataframe.\n",
            "\n",
            "Train: (80, 7) Test: (20, 7)\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Loss moyen = 1.9269\n",
            "[Epoch 2] Loss moyen = 1.7804\n",
            "[Epoch 3] Loss moyen = 1.5857\n",
            "[Epoch 4] Loss moyen = 1.3377\n",
            "[Epoch 5] Loss moyen = 1.0113\n",
            "[Epoch 6] Loss moyen = 0.6938\n",
            "[Epoch 7] Loss moyen = 0.4478\n",
            "[Epoch 8] Loss moyen = 0.2799\n",
            "[Epoch 9] Loss moyen = 0.1583\n",
            "[Epoch 10] Loss moyen = 0.0940\n",
            "[Epoch 11] Loss moyen = 0.0707\n",
            "[Epoch 12] Loss moyen = 0.0551\n",
            "[Epoch 13] Loss moyen = 0.0418\n",
            "[Epoch 14] Loss moyen = 0.0356\n",
            "[Epoch 15] Loss moyen = 0.0305\n",
            "[Epoch 16] Loss moyen = 0.0274\n",
            "[Epoch 17] Loss moyen = 0.0239\n",
            "[Epoch 18] Loss moyen = 0.0217\n",
            "[Epoch 19] Loss moyen = 0.0197\n",
            "[Epoch 20] Loss moyen = 0.0171\n",
            "[Epoch 21] Loss moyen = 0.0162\n",
            "[Epoch 22] Loss moyen = 0.0143\n",
            "[Epoch 23] Loss moyen = 0.0133\n",
            "[Epoch 24] Loss moyen = 0.0121\n",
            "[Epoch 25] Loss moyen = 0.0118\n",
            "[Epoch 26] Loss moyen = 0.0110\n",
            "[Epoch 27] Loss moyen = 0.0103\n",
            "[Epoch 28] Loss moyen = 0.0101\n",
            "[Epoch 29] Loss moyen = 0.0095\n",
            "[Epoch 30] Loss moyen = 0.0085\n",
            "[Epoch 31] Loss moyen = 0.0084\n",
            "[Epoch 32] Loss moyen = 0.0076\n",
            "[Epoch 33] Loss moyen = 0.0066\n",
            "[Epoch 34] Loss moyen = 0.0067\n",
            "[Epoch 35] Loss moyen = 0.0066\n",
            "[Epoch 36] Loss moyen = 0.0063\n",
            "[Epoch 37] Loss moyen = 0.0060\n",
            "[Epoch 38] Loss moyen = 0.0056\n",
            "[Epoch 39] Loss moyen = 0.0054\n",
            "[Epoch 40] Loss moyen = 0.0053\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         AGG       0.20      0.20      0.20         5\n",
            "        ARTH       0.00      0.00      0.00         3\n",
            "        COMP       0.00      0.00      0.00         3\n",
            "      LOOKUP       0.00      0.00      0.00         4\n",
            "        Next       0.00      0.00      0.00         1\n",
            "       SUPER       0.33      0.25      0.29         4\n",
            "       other       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.10        20\n",
            "   macro avg       0.08      0.06      0.07        20\n",
            "weighted avg       0.12      0.10      0.11        20\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Modèle sauvegardé dans semantic_classifier_distilbert/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForSequenceClassification\n",
        ")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"subset_labeled.tsv\", sep=\"\\t\")\n",
        "print(\"Fichier principal chargé :\", df.shape)\n",
        "\n",
        "\n",
        "def load_and_flatten_context(path):\n",
        "    if pd.isna(path):\n",
        "        return \"\"\n",
        "\n",
        "    path = path.strip()\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[WARNING] Chemin introuvable : {path}\")\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        ctx_df = pd.read_csv(path, sep=\"\\t\")\n",
        "        # On aplatit en \"col1: val1 ; col2: val2 ; ...\"\n",
        "        flattened_rows = []\n",
        "        for _, row in ctx_df.iterrows():\n",
        "            txt = \" ; \".join([f\"{col}: {row[col]}\" for col in ctx_df.columns])\n",
        "            flattened_rows.append(txt)\n",
        "\n",
        "        return \" || \".join(flattened_rows)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Impossible de lire {path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Aplatissement des contextes…\")\n",
        "df[\"context\"] = df[\"context\"].str.replace(\".csv\", \".tsv\")\n",
        "df[\"flat_context\"] = df[\"context\"].apply(load_and_flatten_context)\n",
        "df[\"full_text\"] = df[\"utterance\"] + \" | CONTEXT: \" + df[\"flat_context\"]\n",
        "\n",
        "print(\"Contexte aplati ajouté au dataframe.\\n\")\n",
        "\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=256):\n",
        "        self.texts = df[\"full_text\"].tolist()\n",
        "        self.labels = df[\"label\"].astype(\"category\")\n",
        "        self.label_ids = self.labels.cat.codes\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = dict(enumerate(self.labels.cat.categories))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.label_ids.iloc[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "train_ds = TextDataset(train_df, tokenizer)\n",
        "test_ds = TextDataset(test_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=8)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=len(train_df[\"label\"].unique()),\n",
        "    problem_type=\"single_label_classification\"\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "\n",
        "# Entraînement et eval\n",
        "\n",
        "EPOCHS = 40\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Loss moyen = {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "preds, gold = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        labels = batch[\"labels\"]\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits.cpu()\n",
        "\n",
        "        preds.extend(logits.argmax(dim=1).tolist())\n",
        "        gold.extend(labels.tolist())\n",
        "\n",
        "label_names = train_df[\"label\"].astype(\"category\").cat.categories\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(gold, preds, target_names=label_names))\n",
        "\n",
        "model.save_pretrained(\"semantic_classifier_distilbert\")\n",
        "tokenizer.save_pretrained(\"semantic_classifier_distilbert\")"
      ]
    }
  ]
}